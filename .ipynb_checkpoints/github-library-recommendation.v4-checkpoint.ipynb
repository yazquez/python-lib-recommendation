{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #8B008B; font-weight: bold; font-family: Arial; font-size: 3em;\">Sistema de recomendación de librerías Python</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:#4B0082; font-family: Arial; font-size: 2em;\">José Pérez Yázquez</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:#4B0082; font-family: Arial; font-size: 1.5em;\">Octubre de 2017</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #12297c; font-family: Arial; font-size: 3em;\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El presente proyecto implementa una propuesta de recomendador de librerías Python, la idea es que dado un proyecto que estemos desarrollando, nos recomiende librerías que otros proyectos similares han usado. Para ello usa como \"corpus\" la BBDD por excelencia de código, **Github**, y como medida de similitud sus descripciones y las librerías que ya compartan.\n",
    "\n",
    "El proyecto consta de dos fases, en la primera de ellas obtendremos todos los metadatos de los proyectos usando la API Rest de Github y en la segunda parte, basándonos en esos metadatos construiremos el sistema de recomendación.\n",
    "\n",
    "Aunque el objetivo del proyecto es procesar proyectos **Python**, sería fácilmente extensible para que procesara proyectos implementados en otros lenguajes. Por ejemplo, si quisiéramos hacer un sistema de recomendación de librerías **R**, solo sería necesario modificar la expresión regular que busca los **import** para que buscase la instrucción **library**.\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #12297c; font-family: Arial; font-size: 3em;\">Carga y procesamiento de datos</span>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta fase se divide a su vez en cuatro subfases:\n",
    " - Recopilación de la información de los proyectos\n",
    " - Clonado de los proyectos\n",
    " - Procesamiento de los proyectos clonados\n",
    " - Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recopilación de la información de los proyectos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la API Rest de Github descarga la información, metadatos, de todos los proyectos que cumplen una serie de criterios, estos serán los siguientes:\n",
    "  - Identificados como proyectos Python\n",
    "  - Creado entre el 1 de Enero de 2012 y el 31 de Octubre de 2017\n",
    "  - Con un mínimo de 10 estrellas\n",
    "\n",
    "Toda la información es almacenada en MongoDB concretamente en una colección llamada **projects** de una BD llamada **github**, la información almacenada es la siguiente:\n",
    " - **id**: Identificador generado por Github\n",
    " - **name**: Nombre del proyecto\n",
    " - **full_name**: Nombre ampliado del proyecto\n",
    " - **created_at**: Fecha de creación\n",
    " - **git_url**: Url al repositorio (es lo que se usa cuando se quiere clonar)\n",
    " - **description**: Descripción del proyecto\n",
    " - **language**: Lenguaje de programación del proyecto (Java, Python, Scala, etc)\n",
    " - **stars**: Número de estrellas del proyecto, nos da una medida de la *populariodad* del proyecto\n",
    " - **library**: Lista con las librerías que el proyecto usa, en principio estará vacía y se alimentará cuando se procese el proyecto\n",
    " - **readme_txt**: Almacenará el texto de los ficheros README de los proyectos.\n",
    " - **readme_language**: Indica el idioma en el que está redactado el fichero README\n",
    " - **readme_words**: Descomposición del fichero README en palabras con significado semántico, será la información base para el sistema de recomendación\n",
    " - **raw_data**: Almacena toda la información del proyecto sin procesar por si más adelante es necesario algún dato que en estos momentos no parece relevante.\n",
    " \n",
    "Además se incluirán un par de propiedades de uso interno\n",
    "\n",
    " - **library_lines**: Lineas de los ficheros **py** en los que se detectó la importación de una librería. De uso interno\n",
    " - **pipeline_status**: Algunas de las operaciones que se realizan pueden ser muy costosas en cuanto al tiempo que consumen, por ello se lleva para cada proyecto el registro del estado en el que está. \n",
    "\n",
    "Cuando se usa la API de Github sin autenticación existen una serie de límites que no podemos superar, el primero es que el número máximo de resultados que cualquier consulta puede devolver\n",
    "es de 1000, para salvar este impedimento, vamos a realizar sucesivas consultas restringiendo las llamadas a un solo día del intervalo que vamos a cubrir.\n",
    "\n",
    "Por otra parte, existe otra restricción en el uso de la API, el número de llamadas por minuto está limitado a 60, podríamos incrementar esta cantidad si las llamadas son autenticadas,\n",
    "esto es usar por ejemplo un \"client ID\" y \"secret\"  como partes de la consulta, pero vamos abordar una estrategia diferente. Se irán realizando las consultas sin atender a los límites y cuando ese límite se supere, se suspenderá el proceso durante 61 segundos y posteriormente se reanudará\n",
    "por el punto en el que iba.\n",
    "\n",
    "Si se desea consultar los límites comentados, podemos hacer una consulta, por ejemplo usando curl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "curl -i https://api.github.com/users/octocat\n",
    "\n",
    "HTTP/1.1 200 OK\n",
    "\n",
    "Date: Mon, 01 Jul 2017 17:27:06 GMT\n",
    "\n",
    "Status: 200 OK\n",
    "\n",
    "X-RateLimit-Limit: 60\n",
    "\n",
    "X-RateLimit-Remaining: 56\n",
    "\n",
    "X-RateLimit-Reset: 1372700873\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la cabecera de la respuesta nos muestra los límites con el valor actual:\n",
    "\n",
    " - **X-RateLimit-Limit**       Número máximo de solicitudes que se le permite hacer por hora.\n",
    " - **X-RateLimit-Remaining**   Número de solicitudes restantes en la ventana de límite de velocidad actual.\n",
    " - **X-RateLimit-Reset**       Hora a la que se restablece la ventana de límite de velocidad actual en segundos UTC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar vamos a cargar las librerías necesarias para esta fase del proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from datetime import date, timedelta\n",
    "from time import sleep\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a definir una serie de constantes que serán usadas en el código, de esta manera podremos cambiar facilimente alguno de estos valores.\n",
    "El significado de cada una de ellas es el siguiente:\n",
    " - **MIN_STARTS**: Número mínimo de estrellas que tiene que tener un proyecto para ser considerado\n",
    " - **START_DATE**: Fecha de inicio del intervalo, haciendo referencia a la fecha de creación del proyecto en Github\n",
    " - **END_DATE**: Fecha de fin del intervalo\n",
    " - **URL_PATTERN**: Patrón de la URL para la consulta a la API de Github, podemos ver que filtramos por lenguaje (Python), por fecha de creación (la suministraremos durante la ejecución) y por numero de estrellas (referido a la constante comentada anteriormente).\n",
    " - **ROOT_PATH**: Directorio raiz donde clonaremos los repositorios (debe existir antes de empezar a clonar los proyectos).\n",
    " - **CLONE_COMMAND**: Comando usado para clonar los repositorios\n",
    " - **LIBRARY_PATTERN**: Patrón que cumplen las líneas de código en las que se importa una librería\n",
    " - **FILE_LANGUAGE_EXTENSION**: Extensión de los ficheros para el lenguaje a procesar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la introducción deciamos que el sistema podrías ser extensible a otros lenguajes más allá de Python, para ello tendríamos que cambiar las dos últimas constantes a las apropiadas al lenguaje que queramos procesar, por ejemplo para **Java**, la última constante valdría **\".java\"** y para **R** valdría **\".r\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_STARTS = 10\n",
    "START_DATE = date(2017, 8, 1)\n",
    "END_DATE = date(2017, 8, 2)\n",
    "URL_PATTERN = 'https://api.github.com/search/repositories?q=language:Python+created:{0}+stars:>={1}&type=Repositories'\n",
    "\n",
    "CLONE_COMMAND = \"git clone {0} {1}\"\n",
    "ROOT_PATH = \"d:/tfm/tmp\"\n",
    "\n",
    "LIBRARY_PATTERN = '(?m)^(?:from[ ]+(\\S+)[ ]+)?import[ ]+([\\S,\\s]+)(\\n)$'\n",
    "FILE_LANGUAGE_EXTENSION = \".py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se ha comentado anteriormente, vamos a usar MongoDB como tecnología de persistencia, vamos a definir una funcion que nos devuelva una referencia a la colección de proyectos con la que estamos trabajando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repository_projects():\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client.github_t5\n",
    "    return db.projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función es la que realmente hace las llamadas a la API, tendrá como entrada la fecha de creación de los proyectos, usará la URL que hemos definicdo como constante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projects_by_date(date):\n",
    "    print(\"Processing date\", date)\n",
    "    url_pattern = URL_PATTERN\n",
    "    url = url_pattern.format(date, MIN_STARTS)\n",
    "    response = requests.get(url)\n",
    "    if (response.ok):\n",
    "        response_data = json.loads(response.content.decode('utf-8'))['items']\n",
    "        for project in response_data:\n",
    "            insert_project(project)\n",
    "    else:\n",
    "        response.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuanción definimos otra función auxiliar, que dada la información de la respuesta de la API, seleccionamos los atributos que vamos a necesitar y los almacenamos como un documento Mongo. Adicionalmente crea las propiedades mencionadas previamente (readme_txt,library,etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_project(github_project):\n",
    "    if repository_projects.find_one({\"id\": github_project[\"id\"]}):\n",
    "        print(\"Project {0} is already included in the repository\".format(github_project[\"name\"]))\n",
    "    else:\n",
    "        project = {\n",
    "            'id': github_project[\"id\"],\n",
    "            'name': github_project[\"name\"],\n",
    "            'full_name': github_project[\"full_name\"],\n",
    "            'created_at': github_project[\"created_at\"],\n",
    "            'git_url': github_project[\"git_url\"],\n",
    "            'description': github_project[\"description\"],\n",
    "            'language': github_project[\"language\"],\n",
    "            'stars': github_project[\"stargazers_count\"],\n",
    "            'readme_txt': \"\",\n",
    "            'readme_language': None,\n",
    "            'readme_words': [],\n",
    "            'library': [],            \n",
    "            'raw_data': github_project,\n",
    "            'pipeline_status': 'INITIAL',\n",
    "            'library_lines': [],\n",
    "        }\n",
    "        repository_projects.insert(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente ejecutamos el programa que, usando las funciones anteriormente definidas, descarga la información de los proyectos y la inserta en la BBDD, podemos observar que descompone las llamadas para traer en cada una de ellas solo los proyectos de un determinado dia y que *\"gestiona\"* las restricciones que tenemos en cuanto a llamada en la ventana de tiempo actual. \n",
    "\n",
    "En primer lugar recuperamos la colección en la que vamos a insertar los proyectos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_projects = get_repository_projects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Cargamos los proyectos en la colección, aplicamos las técnicas descritas para salvar las restricciones que nos impone la API de Github (número de items devuelto por cada consulta y número de llamadas por minuto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing date 2017-08-01\n",
      "Processing date 2017-08-02\n"
     ]
    }
   ],
   "source": [
    "for project_create_at in [START_DATE + timedelta(days=x) for x in range((END_DATE - START_DATE).days + 1)]:\n",
    "    try:\n",
    "        get_projects_by_date(project_create_at)\n",
    "    except:\n",
    "        print(\">> Reached call limit, waiting 61 seconds...\")\n",
    "        sleep(61)\n",
    "        get_projects_by_date(project_create_at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto tendriamos cargada toda la información que necesitamos de Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clonado de los proyectos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para analizar las librerías que un proyecto utiliza, un camino podría ser buscar los ficheros **requirements** de cada proyecto y parsear esa información, pero dicho fichero no está presente en una gran cantidad de proyectos, por lo que se ha optado por un camino un poco más **radical**.\n",
    "\n",
    "Usando la información cargada en el paso anterior, fundamentalmente la propiedad **git_url**, vamos a clonar cada proyecto en local para posteriormente procesarlos.\n",
    "\n",
    "**NOTAS**: \n",
    "* El directorio ROOT_PATH debe existir antes de lanzar el siguiente script\n",
    "* Nos aseguramos que el directorio no existe, porque este código, dado que puede tardar bastante, lo podemos ejecutar en diferentes momentos, es decir llegado un punto se podría abortar el proceso y lanzarlo más adelante sin que se resienta por ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning project defcon25-public ...\n",
      "Cloning project MachineLearningAction ...\n",
      "Cloning project isf ...\n",
      "Cloning project Deep-Image-Matting ...\n",
      "Cloning project TWindbg ...\n",
      "Cloning project visimportance ...\n",
      "Cloning project vulcan ...\n",
      "Cloning project WAF_Bypass_Helper ...\n",
      "Cloning project programmable-agents_tensorflow ...\n",
      "Cloning project keras-transform ...\n",
      "Cloning project django_rest_example ...\n",
      "Cloning project SkySpyWatch ...\n",
      "Cloning project pypaperbak ...\n",
      "Cloning project QQ_zone ...\n",
      "Cloning project Imports-in-Python ...\n",
      "Cloning project kinetics-i3d ...\n",
      "Cloning project OSINT-SPY ...\n",
      "Cloning project cifar-10-cnn ...\n",
      "Cloning project neural_factorization_machine ...\n",
      "Cloning project webcrawler ...\n",
      "Cloning project solving-minesweeper-by-tensorflow ...\n",
      "Cloning project parseNTFS ...\n",
      "Cloning project attentional_factorization_machine ...\n",
      "Cloning project ChatGirl ...\n",
      "Cloning project Semantic_Segmentation ...\n",
      "Cloning project captcha-svm ...\n",
      "Cloning project pytorch-fitmodule ...\n",
      "Cloning project trafaret ...\n",
      "Cloning project privacy-protocols ...\n",
      "Cloning project lp-diet ...\n",
      "Cloning project repotoddy ...\n",
      "Cloning project minimal_flight_search ...\n",
      "Cloning project deeppavlov ...\n",
      "Cloning project wx_robot_example ...\n",
      "Cloning project rfcat-firsttry ...\n",
      "Cloning project new-pac ...\n"
     ]
    }
   ],
   "source": [
    "os.chdir(ROOT_PATH)\n",
    "\n",
    "for project in repository_projects.find({'pipeline_status':'INITIAL'}):\n",
    "    print(\"Cloning project\", project['name'], \"...\")\n",
    "    path = ROOT_PATH + \"/\" + str(project[\"id\"])\n",
    "    if not os.path.isdir(path):\n",
    "        os.system(CLONE_COMMAND.format(project[\"git_url\"], project[\"id\"]))\n",
    "    project['pipeline_status'] = 'CLONED'\n",
    "    repository_projects.update({'_id': project['_id']}, {\"$set\": project}, upsert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "En este punto tendríamos clonados todos los repositorios con los que construiremos el sistema de recomendación.\n",
    "\n",
    "**NOTA**: La cantidad de datos descargados supera los 1.3Tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de los proyectos clonados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta fase, partiendo de los ficheros ya clonados en local, se identifican los ficheros README y se almacenan en la BBDD, por otra parte, se recorren línea por línea los ficheros Python y, usando expresiones regulares, se extraen las librerías que se están usando para almacenarlas también en la BBDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comenzamos definiendo una función que dado un fichero python (extensión **.py**), lo recorre linea por linea y, usando expresiones regulares, buscamos todas las librerias que se estén usando, de forma esquemática estas son las acciones que se llevan a cabo:\n",
    "\n",
    " - Contemplar la posibilidad de que lo devuelto por la expresión regular sea una lista de librerías (import lib1,lib2,lib), en cuyo caso descomponemos la lista en sus librerías individuales\n",
    " - Se excluyen las librerías que empiezan por \"**.**\" ya que se trata de librerías internas de los proyectos\n",
    " - Se contempla que la librería tenga un alias eliminándolo\n",
    " - Se queda solo con el paquete principal\n",
    " - Se limpian espacios en blanco\n",
    "\n",
    "Almacenamos esta lista en la propiedad **library** del proyecto en cuestión, también almacenamos la lista de líneas que declaran los imports por si en el futuro necesitamos volver a procesarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_python_file(project, file_path):\n",
    "    def process_expression(item):\n",
    "        def insert(lib):\n",
    "            lib = lib.strip()\n",
    "            if '.' in lib:\n",
    "                # Solo nos quedamos con el paquete principal\n",
    "                lib = lib.split('.')[0]\n",
    "            if not lib in library:\n",
    "                library.append(lib)\n",
    "\n",
    "        if not item.startswith('.'):\n",
    "            if ',' in item:\n",
    "                [insert(lib) for lib in item.split(',')]\n",
    "            elif \" as \" in item:\n",
    "                insert(item.split(\" as \")[0])\n",
    "            else:\n",
    "                insert(item)\n",
    "\n",
    "    library = project['library']\n",
    "    library_lines = project['library_lines']\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            match = re.search(LIBRARY_PATTERN, line)\n",
    "            if match:\n",
    "                library_lines.append(line)\n",
    "                if match.group(1) != None:\n",
    "                    process_expression(match.group(1))\n",
    "                else:\n",
    "                    process_expression(match.group(2))\n",
    "\n",
    "    project['library'] = library\n",
    "    project['library_lines'] = library_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otra parte, definimos también una función que dado un fichero **README** lo lea y almacena en la propiedad **readme_txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_readme_file(project, file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        project['readme_txt'] = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente recorremos los repositorios no procesados, como comentabamos anteriormente este proceso permite lanzarlo reiteradas veces, para cada repositorio analizamos su fichero **README** y cada uno de los ficheros Python para extraer las librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing project defcon25-public\n",
      "Processing project MachineLearningAction\n",
      "Processing project isf\n",
      "Processing project Deep-Image-Matting\n",
      "Processing project TWindbg\n",
      "Processing project visimportance\n",
      "Processing project vulcan\n",
      "Processing project WAF_Bypass_Helper\n",
      "Processing project programmable-agents_tensorflow\n",
      "Processing project keras-transform\n",
      "Processing project django_rest_example\n",
      "Processing project SkySpyWatch\n",
      "Processing project pypaperbak\n",
      "Processing project QQ_zone\n",
      "Processing project Imports-in-Python\n",
      "Processing project kinetics-i3d\n",
      "Processing project OSINT-SPY\n",
      "Processing project cifar-10-cnn\n",
      "Processing project neural_factorization_machine\n",
      "Processing project webcrawler\n",
      "Processing project solving-minesweeper-by-tensorflow\n",
      "Processing project parseNTFS\n",
      "Processing project attentional_factorization_machine\n",
      "Processing project ChatGirl\n",
      "Processing project Semantic_Segmentation\n",
      "Processing project captcha-svm\n",
      "Processing project pytorch-fitmodule\n",
      "Processing project trafaret\n",
      "Processing project privacy-protocols\n",
      "Processing project lp-diet\n",
      "Processing project repotoddy\n",
      "Processing project minimal_flight_search\n",
      "Processing project deeppavlov\n",
      "Processing project wx_robot_example\n",
      "Processing project rfcat-firsttry\n",
      "Processing project new-pac\n"
     ]
    }
   ],
   "source": [
    "for project in repository_projects.find({'pipeline_status':'CLONED'}):\n",
    "    try:\n",
    "        path = ROOT_PATH + \"/\" + str(project[\"id\"])\n",
    "        if os.path.isdir(path):\n",
    "            print(\"Processing project\", project[\"name\"])\n",
    "            for root, dirs, files in os.walk(path):\n",
    "                for file in files:\n",
    "                    try:\n",
    "                        if file.endswith(FILE_LANGUAGE_EXTENSION):\n",
    "                            process_python_file(project, os.path.join(root, file))\n",
    "                        else:\n",
    "                            if file.lower().startswith(\"readme.\"):\n",
    "                                process_readme_file(project, os.path.join(root, file))\n",
    "                    except:\n",
    "                        pass\n",
    "            project['pipeline_status'] = 'PROCESSED'\n",
    "            repository_projects.update({'_id': project['_id']}, {\"$set\": project}, upsert=False) \n",
    "    except:\n",
    "        print(\"Error procesing project {0} [{1}] - {2}\".format(project['id'], project['name'], sys.exc_info()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detección del idioma del proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que uno de los pilares del sistema de detección es el Procesamiento del Lenguaje Natural, uno de los primeros pasos es detectar el idioma en el que está redactado el fichero **README** fuente de nuestros textos. Dado que la mayoría de proyectos están redactados en Ingles, y en aras de simplificar el modelo, vamos a eliminar todos los proyectos que no estén descritos en este idioma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello vamos a usar la función **detect** de la librería **langdetect**, implementaremos un *wrapper* sobre ella que tendrá en cuenta los posibles errores así como la posibildad de que el fichero README esté vacío."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def detect_language(project):\n",
    "    language = None\n",
    "    try:\n",
    "        if len(project['readme_txt']):\n",
    "            language = detect(project['readme_txt'])\n",
    "    except:\n",
    "        pass\n",
    "    return (language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los proyectos que, por un motivo un otro, no estén en **Inglés**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_not_english = [project for project in list(repository_projects.find()) if detect_language(project) != 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que el número de proyectos que vamos a borrar no es muy elevado con respecto al total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English proyects number  : 9\n",
      "Total number of proyects : 36\n"
     ]
    }
   ],
   "source": [
    "print(\"English proyects number  :\", len(projects_not_english))\n",
    "print(\"Total number of proyects :\", repository_projects.count() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente borramos los proyectos que no nos interesan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for project in projects_not_english:\n",
    "    repository_projects.delete_one({'id': project[\"id\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos el número de proyectos que han quedado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of proyects : 27\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of proyects :\", repository_projects.count() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos la base de los metadatos que vamos a usar, el siguiente paso es procesarlos de cara a tenerlos preparados para su uso directo por parte de los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyectos\n",
    "\n",
    "En cuanto a lo que a los proyectos se refiere, procesamos las descripciones quedándonos con las palabras diferentes que encontramos en cada una de ellas, eliminamos puntuaciones, números y palabras irrelevantes como nombres personales y palabras comunes que no aportan significado (denominados \"stop words\"), esto evitará que se formen tópicos en torno a ellos\n",
    "\n",
    "Además, utilizamos el stemmer Snowball, también llamado Porter2 stemmer, para detectar palabras similares presentes en diferentes formatos (eliminar sufijo, prefijo, etc.). Snowball es un lenguaje desarrollado por M.F. Porter, para definir de forma eficiente stemmers. Este algoritmo de derivación es el más utilizado en el dominio del procesamiento del lenguaje natural.\n",
    "\n",
    "Para hacer el procesamiento usaremos la librería **nltk** (Natural Language Toolkit), proporciona un gran número de métodos que cubren diferentes temas en el dominio de los datos del lenguaje humano, como la clasificación, derivación, etiquetado, análisis y razonamiento semántico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función que dado un texto lo descompone en las palabras con significado que lo componen. Vamos a excluir los siguientes tipos de palabras:\n",
    "\n",
    "- Nombres propios\n",
    "- Palabras consideradas como \"Stop Words\"\n",
    "- Números\n",
    "\n",
    "Adicionalmente aplicamos el proceso de \"steamming\" que comentabamos anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    def add_word(word):\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and not word.replace('.', '').isdigit():\n",
    "            words.append(stemmer.stem(word))\n",
    "\n",
    "    words = []\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(tokenizer.tokenize(text))):\n",
    "        # nltk.word_tokenize    devuelve la lista de palabras que forman la frase (tokenización)\n",
    "        # nltk.pos_tag          devuelve el part of speech (categoría) correspondiente a la palabra introducida\n",
    "        # nltk.ne_chunk         devuelve la etiqueta correspondiente al part of speech (POC)\n",
    "        try:\n",
    "            if chunk.label() == 'PERSON':\n",
    "                # PERSON es un POC asociado a los nombres propios, los cuales no vamos a añadir\n",
    "                pass\n",
    "            else:\n",
    "                for c in chunk.leaves():\n",
    "                    add_word(c[0])\n",
    "        except AttributeError:\n",
    "            add_word(chunk[0])\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Librerías\n",
    "\n",
    "En cuanto a las librerías solo vamos tener en cuenta las librerías de \"**terceros**\", es decir vamos a excluir las librerías definidas en los propios proyectos y las librerías del sistema, las primeras porque obviamente no las van a tener otros proyectos y lo único que haría sería incluir ruido y las segundas, las del sistema, por ser demasiado comunes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder catalogar las librerías de terceros, vamos a usar como referencia las librerías disponibles en [PyPi](https://pypi.python.org/pypi). Realizaremos un proceso de scraping sobre su web, almacenaremos las librerías un una colección de nuestra BBDD Mongo.\n",
    "\n",
    "**NOTA**: Vamos a sustituir el caracter \"**-**\" por \"**_**\", esto se hace porque nos encontraremos casos en los que la librería se llame algo como **some-word** y la instalación se deba hacer con ese nombre **pip install some-word**, sin embargo, cuando la encontremos en el código pasará a llamarse **some_word**, es decir encontraremos algo como **import some_word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "PYPI_LIST_URL = 'https://pypi.python.org/simple/'\n",
    "\n",
    "def get_repository_library():\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client.github_t5\n",
    "    return db.library\n",
    "\n",
    "def insert_library(library_name):\n",
    "    library_name = library_name.lower().replace(\"-\", \"_\")\n",
    "    if repository_library.find_one({\"name\": library_name}):\n",
    "        print(\"Library {0} is already included in the repository\".format(library_name))\n",
    "    else:\n",
    "        library = {\n",
    "            'name': library_name\n",
    "        }\n",
    "        repository_library.insert(library)\n",
    "\n",
    "repository_library = get_repository_library()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = requests.get(PYPI_LIST_URL)\n",
    "\n",
    "for line in content.text.split('\\n'):\n",
    "    match = re.search(\"'>([\\w\\-\\.]+)\", line)\n",
    "    if match:\n",
    "        insert_library(match.group(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir una función que filtre las librerías que no pertenezcan a la colección creada en el paso anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_library(library):\n",
    "    library_processed = []\n",
    "    for lib in library:\n",
    "        if repository_library.find_one({\"name\": lib}):\n",
    "            library_processed.append(lib)\n",
    "    return library_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación recorremos los proyectos aplicándole tanto el procesamiento a los ficheros **README** como a las **librerías**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing defcon25-public\n",
      "Processing isf\n",
      "Processing Deep-Image-Matting\n",
      "Processing TWindbg\n",
      "Processing visimportance\n",
      "Processing vulcan\n",
      "Processing WAF_Bypass_Helper\n",
      "Processing programmable-agents_tensorflow\n",
      "Processing keras-transform\n",
      "Processing django_rest_example\n",
      "Processing SkySpyWatch\n",
      "Processing pypaperbak\n",
      "Processing Imports-in-Python\n",
      "Processing kinetics-i3d\n",
      "Processing OSINT-SPY\n",
      "Processing cifar-10-cnn\n",
      "Processing neural_factorization_machine\n",
      "Processing attentional_factorization_machine\n",
      "Processing ChatGirl\n",
      "Processing Semantic_Segmentation\n",
      "Processing pytorch-fitmodule\n",
      "Processing trafaret\n",
      "Processing privacy-protocols\n",
      "Processing lp-diet\n",
      "Processing repotoddy\n",
      "Processing deeppavlov\n",
      "Processing rfcat-firsttry\n"
     ]
    }
   ],
   "source": [
    "for project in repository_projects.find({'pipeline_status':'PROCESSED'}):\n",
    "    try:\n",
    "        print(\"Processing\",project[\"name\"])\n",
    "        project['readme_words'] = get_words(project['readme_txt'])\n",
    "        project['library'] = process_library(project['library'])\n",
    "        project['pipeline_status'] = 'DONE'\n",
    "    \n",
    "        repository_projects.update({'_id': project['_id']}, {\"$set\": project}, upsert=False)\n",
    "    except:\n",
    "        print(\"Error procesing project {0} [{1}] - {2}\".format(project['id'], project['name'], sys.exc_info()[0]))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente eliminaremos de nuestro repositorio los proyectos que no tienen librerías o que no tienen ningun elemento en su lista de palabras (la descomposición de los ficheros **readme**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for project in repository_projects.find({'pipeline_status':'DONE'}):\n",
    "    if len(project['library']) == 0 or len(project['readme_words']) == 0:\n",
    "        print(\"Deleting\",project[\"name\"])\n",
    "        repository_projects.delete_one({'id': project[\"id\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #12297c; font-family: Arial; font-size: 3em;\">Implementación de modelos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras la fase anterior, tendríamos una colección MongoDB con cada uno de los proyectos de nuestro \"pre-corpus\" con la lista de librerías que cada proyecto usa, así como su descripción **\"extendida\"** extraída de su fichero **README**. Estaríamos en disposición por tanto de comenzar la implementación de nuestro recomendador.\n",
    "\n",
    "Vamos a implementar dos versiones del recomendador, una que buscará similitudes basándose en las descripciones de los proyectos y otra que se basará en las librerías que cada proyecto usa para determinar proyectos similares.\n",
    "Para la primera de las opciones se usará un modelo **LSA** (Latent Semantic Analysis) usando procesamiento de lenguaje natural, para la segunda se usará un simple **\"Count Vectorizer\"**, dado que la aproximación anterior no es adecuada para documentos con pocos términos, en nuestro caso un documento sería el listado de las librerías usadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comenzamos importando las librerías que vamos a usar en esta fase del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Slow version of gensim.models.doc2vec is being used\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import itertools\n",
    "import gensim\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pymongo import MongoClient\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una funcion para cargar los datos de los proyectos, entre los datos a cargar está el fichero **readme** y las librerías usadas, datos que serán los que usemos para calcular las similitudes entre los proyectos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_projects(max_projects=50000):\n",
    "    def get_repository():\n",
    "        client = MongoClient('localhost', 27017)\n",
    "        db = client.github_t5\n",
    "        return db.projects\n",
    "    projects_repository = get_repository()\n",
    "    return list(projects_repository.find({'pipeline_status':'DONE'}).limit(max_projects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando la funcíon anterior cargamos los datos, dado que el proceso es bastante pesado solo cargaremos un subconjunto de las mismas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = load_projects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de ejemplo, mostramos las primeras 5 librerías de uno de los proyectos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parser', 'pyparsing', 're', 'idc', 'ctypes']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects[0]['library'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función que construye una lista con todas las descripciones de todos los proyectos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_readme(projects):\n",
    "    texts = []\n",
    "    [texts.append(project['readme_words']) for project in projects]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos la función que acabamos de definir, tendremos una lista de listas, en la que para cada proyecto tendremos las palabras que lo definen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_from_readme = get_texts_from_readme(projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de ejemplo, mostramos las 5 primeras entradas de uno de los proyectos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['make', 'build', 'dynam', 'librari', 'window']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_from_readme[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y una parte del contenido del fichero README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## Usage\\n\\nRun `make` to build the dynamic library, Windows DLLs, and wrapper for Python. The _Makefile_ uses MinGW to cross compile the Windows DLLs, so you will get errors if you don't have it installed. If you are on macOS, it is sufficient to `brew install mingw-w64`.\\n\\nYou can do a basic test of the disassembler by running _test.py_. It will attempt to disassemble every 4 byte sequency in _test.bin_.\\n\\nIf you want use the IDA plugin, you will need to copy the files to the appropriate directories. Assuming that *IDA_PATH* is your IDA directory, copy the following files:\\n\\n  - riscv_32.dll, riscv_64.dll -> *IDA_PATH/*\\n  - pyriscv.py -> *IDA_PATH/python/*\\n  - ida-plugin/riscv.py -> *IDA_PATH/procs/*\\n\\nThe C library exposes a simple interface:\\n\\n```\\nEXPORT void disassemble(inst_t *inst, uint32_t pc, const uint8_t *buf);\\nEXPORT const char *mnemonics[];\\nEXPORT const char *registers[];\\nEXPORT const unsigned int num_registers;\\n```\\n\\nThe Python wrapper exposes a similar interface:\\n\\n```\\ndisassemble(pc, buf) # returns inst_t structure\\nregisters            # array of register names\\nmnemonics            # array of instruction mnemonics\\n```\\n\\nThe *inst_t* structure contains the results of the disassembly. The fields should be customized for the target architecture and are filled in by the _decode_ functions in _riscv.c_. Every field also defines *used\\\\_field\\\\_name* which is set in _decode_ if that field is initialized.\\n\\nEvery architecture has the following members in *inst_t*:\\n\\n```\\n    uint32_t pc;\\n    const uint8_t *bytes;\\n    unsigned int size;\\n    unsigned int id;\\n    const char *mnemonic;\\n    char str[64];\\n```\\n\\nThe riscv32 architecture defines these additional members (e.g. fields):\\n\\n```\\n    uint8_t opcode;\\n    uint8_t funct3;\\n    uint8_t funct7;\\n    uint8_t rd;\\n    uint8_t rs1;\\n    uint8_t rs2;\\n    int32_t imm;\\n\\n    uint8_t used_opcode;\\n    uint8_t used_funct3;\\n    uint8_t used_funct7;\\n    uint8_t used_rd;\\n    uint8_t used_rs1;\\n    uint8_t used_rs2;\\n    uint8_t used_imm;\\n```\\n\\nT\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projects[0]['readme_txt'][:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo LSA/LSI: Latent Semantic Analysis / Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principios teóricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para identificar la similitud entre los proyectos basándonos en su descripción, entendiéndose como tal su fichero README, utilizamos el \"análisis semántico latente\" (LSA, usando la abreviatura en inglés), que es una técnica ampliamente utilizada en el procesamiento del lenguaje natural. LSA transforma cada texto en un vector, en un espacio de características. En nuestro caso, las características son palabras que ocurren en las descripciones. A continuación, se crea una matriz que contiene todos los vectores: las columnas representan las descripciones de los proyectos y las filas representan palabras únicas. Por consiguiente, el número de filas puede ascender a decenas de miles de palabras. \n",
    "\n",
    "Con el fin de identificar las características relevantes de esta matriz, usaremos la \"descomposición de valores singulares\" (SVD, usando la abreviatura en inglés), que es una técnica de reducción de dimensión, se utiliza para reducir el número de líneas -palabras-, manteniendo y resaltando la similitud entre columnas-descripción -. La dimensión de esta matriz de aproximación se establece mediante un hiperparámetro que es el número de temas, comúnmente llamado como tópicos. En este marco, un tópico consiste en un conjunto de palabras con pesos asociados que definen la contribución de cada palabra a la dirección de este tópico. Basándose en esta matriz de aproximación de baja dimensión, la similitud entre dos columnas -descripciones- se calcula utilizando el coseno del ángulo entre estos dos vectores.\n",
    "\n",
    "\n",
    "**NOTA**: Generalmente LSA y LSI se utilizn indistintamente para referirse al mismo concepto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del diccionario\n",
    " \n",
    "El diccionario está formado por la concatenación de todas las palabras que aparecen en algún resumen de alguno de los proyectos. Básicamente esta función mapea cada palabra única con su identificador. Es decir, si tenemos N palabras, lo que conseguiremos al final es que cada proyecto sea representada mediante un vector en un espacio de N dimensiones.\n",
    " \n",
    "Para ello, partiendo de la lista creada en el paso anterior, usaremos la función **corpora** del paquete **gensim**.\n",
    "\n",
    "El diccionario consiste en una concatenación de palabras únicas de todas las descripciones. Gensim es una biblioteca eficiente para analizar la similitud semántica latente entre documentos.\n",
    "Este módulo implementa el concepto de Diccionario - un mapeo entre palabras y\n",
    "sus entes ids.\n",
    "\n",
    "Los diccionarios pueden ser creados a partir de un corpus y luego pueden ver las frecuencia del documento (eliminación de palabras comunes mediante el método func: `Dictionary.filter_extremes`), guardado / cargado desde el disco (vía: func: `Dictionary.save` y: func:` Dictionary.load`), fusionado con otro diccionario (: func: `Dictionary.merge_with`) etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x18d9c510>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts_from_readme)\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver la longitud del diccionario creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1883 unique tokens: ['isweekend', 'protocol', 'accord', 'question', 'directori']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función **token2i** asigna palabras únicas con sus ids. En nuestro caso, la longitud del diccionario es igual a *N* palabras lo que significa que cada descripción del proyecto será representada a través de un espacio vectorial de *N* dimensiones\n",
    "\n",
    "Mostramos las primeras 10 entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('isweekend', 1425),\n",
       " ('protocol', 193),\n",
       " ('accord', 1377),\n",
       " ('question', 1122),\n",
       " ('directori', 2),\n",
       " ('perform', 955),\n",
       " ('convnet', 1126),\n",
       " ('26817e', 1128),\n",
       " ('us', 282),\n",
       " ('run_exampl', 1567)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.islice(dictionary.token2id.items(), 0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del Corpus\n",
    "\n",
    "Crearemos un corpus con la colección de todos los resúmenes previamente pre-procesados y transformados usando el diccionario. Vamos a convertir los textos a un formato que gensim puede utilizar, esto es, una representación como bolsa de palabras (BoW). Gensim espera ser alimentado con una estructura de datos de corpus, básicamente una lista de \"sparce vectors\", estos constan de pares (id, score), donde el id es un ID numérico que se asigna al término a través de un diccionario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos el numero de elementos únicos que existen en los documentos (los que componen el diccionario) y el numero de textos (correspondientes a las descripciones de los proyectos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens : 1883\n",
      "Number of documents     : 27\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens : %d' % len(dictionary))\n",
    "print('Number of documents     : %d' % len(texts_from_readme))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(dictionary, texts):\n",
    "    return [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(dictionary, texts_from_readme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de ejemplo, mostramos las 5 primeras entradas del primer proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (1, 3), (2, 2), (3, 3), (4, 1)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del modelo TFID\n",
    "\n",
    "Un alto peso en tf-idf se alcanza por una alta frecuencia en un Documento y una baja frecuencia en toda la colección de documentos; los pesos tienden a filtrar términos comunes. Para la creación de este corpus, vamos a usar la función **TfidfModel** del objeto **models** (perteneciente a la librería *gemsim*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf(corpus):\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    return corpus_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = create_tfidf(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En corpus tenemos, para cada project, una lista con sus palabras y el tfidf de cada una. Mostramos las 10 primeras entradas del <primer elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.028236579108337465),\n",
       " (1, 0.07329182662576354),\n",
       " (2, 0.028236579108337465),\n",
       " (3, 0.09281087388977204),\n",
       " (4, 0.03093695796325735),\n",
       " (5, 0.016513052537392683),\n",
       " (6, 0.020624638642171563),\n",
       " (7, 0.04886121775050903),\n",
       " (8, 0.03093695796325735),\n",
       " (9, 0.024430608875254514)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tfidf[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos saber que palabra es cada uno de estos términos podemos consultar el diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w64 , r , ad\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[8],\",\",dictionary[19],\",\", dictionary[88])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinación del número de topicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los elementos que tenemos hasta ahora ya estamos listos para entrenar el modelo LSI/LDA. Pero antes que nada, como dicen los anglosajones \"*el elefante en la habitación*\", **¿Cuál es el número correcto de tópicos?**\n",
    "\n",
    "LSA busca identificar un conjunto de tópicos relacionados las descripciones de los proyectos. El número de estos temas N es igual a la dimensión de la matriz de aproximación resultante de la técnica de reducción de dimensión SVD. Este número es un hiperparámetro que se debe ajustar cuidadosamente, es el resultado de la selección de los N valores singulares más grandes de la matriz del corpus tf-idf. \n",
    "\n",
    "En principio no hay una respuesta correcta o fácil para la pregunta inicial, depende tanto de los datos que tengamos como de la aplicación que queramos dar a estos datos. Por ejemplo, en ciertos casos puede ser útil tener un número pequeño de tópicos, de mara que se puedan interpretar y \"etiquetar\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear una serie de modelos variando el número de tópicos y posteriormente los evaluaremos buscando el número óptimo de tópicos.\n",
    "\n",
    "\n",
    "**NOTA**: Gensim puede provocar cuelgues por un uso excesivo de memoria, así que hay que tener en cuenta esta limitación cuando se indica por ejemplo el número de tópicos. Antes de entrenar su modelo, se puede obtener una estimación aproximada del uso de la memoria usando la siguiente fórmula:\n",
    "\n",
    "</br>\n",
    "\n",
    "<center>**8 bytes** *x* **numero_terminos** *x* **numero_topicos** *x* **3**</center>\n",
    "\n",
    "Así pues, dadas las características de máquina con la que se realizán los cálculos vamos a indicar **120** tópicos como máximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA(k=10)\n",
      "Training LDA(k=20)\n",
      "Training LDA(k=30)\n",
      "Training LDA(k=40)\n",
      "Training LDA(k=50)\n",
      "Training LDA(k=60)\n",
      "Training LDA(k=70)\n",
      "Training LDA(k=80)\n",
      "Training LDA(k=90)\n",
      "Training LDA(k=100)\n",
      "Training LDA(k=110)\n",
      "Training LDA(k=120)\n"
     ]
    }
   ],
   "source": [
    "trained_models = OrderedDict()\n",
    "for num_topics in range(10, 121, 10):\n",
    "    print(\"Training LDA(k=%d)\" % num_topics)\n",
    "    lda = models.LdaMulticore(\n",
    "        corpus_tfidf, id2word=dictionary, num_topics=num_topics, workers=4,\n",
    "        passes=10, iterations=100, random_state=42, \n",
    "        eval_every=None,      # No evalúa la perplejidad del modelo, lleva demasiado tiempo.\n",
    "        alpha='asymmetric',   # Demuestra ser mejor que simétrico en la mayoría de los casos\n",
    "        decay=0.5, offset=64  # Mejores varlores para Hoffman paper\n",
    "    )\n",
    "    trained_models[num_topics] = lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos mostrar uno de los modelos creados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamulticore.LdaMulticore at 0x18d9cef0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_models[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación usando la *Coherencia*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a evaluar cada uno de nuestros modelos de LDA utilizando la coherencia. La **coherencia** es una medida de cuán interpretables son los topicos para los humanos. Se basa en la representación de temas como las N palabras más probables para un topico en particular. Más específicamente, dada la matriz de *topic-term* para LDA, clasificamos cada tema de pesos más altos a más bajos y luego seleccionamos los primeros N términos.\n",
    "\n",
    "La coherencia esencialmente mide cuán similares son estas palabras entre sí. Existen varios métodos para hacerlo, la mayoría de los cuales han sido explorados en el documento [\"Exploring the Space of Topic Coherence Measures\"](https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf). Los autores realizaron un análisis comparativo de varios métodos, correlacionándolos con juicios humanos. El método llamado coherencia \"c_v\" resultó ser el más altamente correlacionado. Este y varios de los otros métodos se han implementado en gensim.models.CoherenceModel. Usaremos esto para realizar nuestras evaluaciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recorremos la lista de modelos y obtenemos la **coherencia** de cada uno de ellos, además llevaremos un registro para determinar cual de ellos obtiene el mejor resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic number: 10 - Coherence: 0.6746561302507453\n",
      "Topic number: 20 - Coherence: 0.6995934153834668\n",
      "Topic number: 30 - Coherence: 0.7215333235188932\n",
      "Topic number: 40 - Coherence: 0.72088870991336\n",
      "Topic number: 50 - Coherence: 0.7273327122959137\n",
      "Topic number: 60 - Coherence: 0.7272353970334672\n",
      "Topic number: 70 - Coherence: 0.7278426205261573\n",
      "Topic number: 80 - Coherence: 0.7363613347398256\n",
      "Topic number: 90 - Coherence: 0.7294358751795041\n",
      "Topic number: 100 - Coherence: 0.7446173646569327\n",
      "Topic number: 110 - Coherence: 0.7391535053844792\n",
      "Topic number: 120 - Coherence: 0.7369847301148149\n",
      "\n",
      "Best model has 100 topics and a coherence of 0.7446173646569327\n"
     ]
    }
   ],
   "source": [
    "best_coherence = 0\n",
    "best_coherence_key = 0\n",
    "for key, value in trained_models.items():\n",
    "    cm = CoherenceModel(model=value, texts=texts_from_readme, dictionary=dictionary, coherence='c_v')\n",
    "    actual_coherence = cm.get_coherence()\n",
    "    print(\"Topic number: {} - Coherence: {}\".format(key,actual_coherence))\n",
    "    if actual_coherence > best_coherence:\n",
    "        best_coherence = actual_coherence\n",
    "        best_coherence_key = key\n",
    "print(\"\\nBest model has {} topics and a coherence of {}\".format(best_coherence_key,best_coherence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prueba nos ha dado como resultado que con **20** tópicos obtenemos los mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finalmente creamos constantes para los valores seleccionados.\n",
    " * **TOTAL_LSA_TOPICS**: Limita el numero de terminos, por supuesto tiene que ver con el tamaño de la muestra, mientras más proyectos tengamos mas terminos tendremos y por tanto la reduccion seria mayor, estamos clusterizando las proyectos en TOTAL_TOPICOS_LSA clusters\n",
    " \n",
    "* **SIMILARITY_THRESHOLD** Umbral de similitud que se debe superar para que dos proyectos se consideren similares (en cualquier caso, posteriormente usaremos los proyectos ordenados por similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_coherence_key = 100\n",
    "TOTAL_LSA_TOPICS = best_coherence_key\n",
    "SIMILARITY_THRESHOLD = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_LSA_TOPICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del modelo LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una serie de funciones auxiliares para ayudarnos en las siguientes operaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_similarities_by_description**\n",
    "\n",
    "Dado un documento (correspondiente a la descripción de un proyecto), nos determina la lista de proyectos que superan el umbral de similitud. Para cada proyecto que supere el umbral, almacenaremos el índice dentro de la matriz de proyectos, para localizarla posteriormente, y el grado de similitud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarities_by_description(model, dictionary, similarity_matrix, doc):\n",
    "    ''' Calcula las similitudes de un documento, expresado este como una lista de palabras'''\n",
    "    project_similarities = defaultdict(float)\n",
    "\n",
    "    # Convertimos el documento al espacio LSI\n",
    "    vec_bow = dictionary.doc2bow(doc)\n",
    "    vec_lsi = model[vec_bow]\n",
    "\n",
    "    similarities = similarity_matrix[vec_lsi]\n",
    "    similarities = sorted(enumerate(similarities), key=lambda item: -item[1])\n",
    "\n",
    "    for sim in similarities:\n",
    "        similarity_project = int(sim[0])\n",
    "        similarity_score = sim[1]\n",
    "        if similarity_score > SIMILARITY_THRESHOLD:\n",
    "            project_similarities[similarity_project] = similarity_score\n",
    "\n",
    "    return (project_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**show_project_similarities**\n",
    "\n",
    "Dada una lista de proyectos similares a uno dado, lo imprime por pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_project_similarities(projects, project_similarities):\n",
    "    for project_id in sorted(project_similarities, key=project_similarities.get, reverse=True)[:10]:\n",
    "         print(\"Project: {0}: - Similarity: {1}\".format(projects[project_id][\"name\"], project_similarities[project_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_library_similarities**\n",
    "\n",
    "Dada una lista de proyectos similares a uno dado y una lista de librerías, retorna las librerías que usan esos proyectos y que no están en la lista pasada por parámetro. Además para cada librería calcula su índice de similitud, basándose en la similitud de los proyectos a los que pertenece. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_library_similarities(projects, project_similarities, project_libraries):\n",
    "    library_similarities=defaultdict(float)\n",
    "    for project_id in sorted(project_similarities, key=project_similarities.get, reverse=True)[:10]:\n",
    "        similarity = project_similarities[project_id]\n",
    "        libraries = projects[project_id][\"library\"]\n",
    "        for library in libraries:\n",
    "            if library not in project_libraries:\n",
    "                library_similarities[library] += similarity\n",
    "    return library_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**show_library_similarities**\n",
    "\n",
    "Dada una lista de proyectos similares a uno dado y una lista de librerías, retorna las librerías que usan esos proyectos y que no están en la lista pasada por parámetro. Además apra cada librería calcula su índice de similitud, basándose en la similitud de los proyectos a los que pertenece. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_library_similarities(library_similarities):\n",
    "    for library in sorted(library_similarities, key=library_similarities.get, reverse=True)[:10]:\n",
    "        print(\"Library: {0}: - Score: {1}\".format(library, library_similarities[library]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ello usaremos la librería **models** que nos ofrece gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=TOTAL_LSA_TOPICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver como influyen las palabras en la determinación de los diferentes tópicos, por ejemplo mostramos los 3 primeros tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "-0.251*\"train\"\n",
      "-0.194*\"model\"\n",
      "-0.141*\"data\"\n",
      "-0.113*\"br\"\n",
      "-0.108*\"kinet\"\n",
      "-0.107*\"rgb\"\n",
      "-0.103*\"dataset\"\n",
      "-0.102*\"ml\"\n",
      "-0.099*\"trafaret\"\n",
      "-0.097*\"repotoddi\"\n",
      "\n",
      "Topic 1:\n",
      "-0.245*\"train\"\n",
      "-0.204*\"vgg\"\n",
      "0.203*\"trafaret\"\n",
      "-0.168*\"pre\"\n",
      "-0.148*\"weight\"\n",
      "0.136*\"repotoddi\"\n",
      "-0.120*\"br\"\n",
      "0.115*\"import\"\n",
      "-0.107*\"model\"\n",
      "0.106*\"osint\"\n",
      "\n",
      "Topic 2:\n",
      "-0.231*\"val_accuraci\"\n",
      "-0.231*\"val_loss\"\n",
      "-0.204*\"epoch\"\n",
      "-0.168*\"accuraci\"\n",
      "-0.168*\"import\"\n",
      "-0.149*\"modul\"\n",
      "-0.144*\"self\"\n",
      "0.142*\"osint\"\n",
      "-0.137*\"trafaret\"\n",
      "-0.128*\"loss\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(model.print_topics(3)):\n",
    "    print('Topic {}:'.format(i))\n",
    "    print(topic[1].replace(' + ', '\\n'))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando nuestro modelo LSI construimos la matriz de similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = gensim.similarities.MatrixSimilarity(model[corpus_tfidf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de concepto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A modo de ejemplo, como prueba de concepto, vamos a determinar los proyectos similares a uno dado. En primer lugar definimos las propiedades de nuestro proyecto **fake**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "poc_library = [ \"logging\", \"time\", \"printer\", \"distutils\", \"scapy\", \"requests\", \"bs4\", \"pysnmp\", \"paramiko\", \"nmap\", \"unittest\" ]\n",
    "poc_readme_words = get_words(\"works on Windows and write down some installation instructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la lista de proyectos similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_similarities_by_description = get_similarities_by_description(model, dictionary, similarity_matrix, poc_readme_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos los 10 proyectos más similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: defcon25-public: - Similarity: 0.7655923366546631\n",
      "Project: deeppavlov: - Similarity: 0.4258296489715576\n"
     ]
    }
   ],
   "source": [
    "show_project_similarities(projects, project_similarities_by_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "El objetivo final no no es encontrar proyectos similares, sino encontrar las librerias que usan esos proyectos similares. Así pues, recorremos esos proyectos e identificamos esas librerías, descartando las que nuestro proyecto ya incluye."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recorremos los 20 proyectos más similares y calculamos el scoring de cada librería usada en esos proyectos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_similarities_by_description = get_library_similarities(projects, project_similarities_by_description, poc_library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos las 10 librerías más usadas por los proyectos similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library: re: - Score: 1.1914219856262207\n",
      "Library: parser: - Score: 0.7655923366546631\n",
      "Library: ctypes: - Score: 0.7655923366546631\n",
      "Library: pyparsing: - Score: 0.7655923366546631\n",
      "Library: idc: - Score: 0.7655923366546631\n",
      "Library: build_utils: - Score: 0.4258296489715576\n",
      "Library: uuid: - Score: 0.4258296489715576\n",
      "Library: tqdm: - Score: 0.4258296489715576\n",
      "Library: fasttext: - Score: 0.4258296489715576\n",
      "Library: nltk: - Score: 0.4258296489715576\n"
     ]
    }
   ],
   "source": [
    "show_library_similarities(library_similarities_by_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Otro acercamiento que podemos realizar es usar como nivel de similitud las propias librerías que un proyecto usa, es decir buscaremos proyectos que usen las mismas librerías que nosotros ya estamos usando y mostraremos otras librerias que esos mismos proyectos estén usando y nuestro proyecto no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso los documentos serán muy pequeños, así que técnicas como LSI/LDA no son apropidas dado que están basados en estadísticas y con documentos pequeños no tendríamos suficiente información, se podrían usar técnicas como\n",
    "[Word network topic model](https://dl.acm.org/citation.cfm?id=2974795), pero en este caso vamos a usar una aproximacion más simple y usaremos un modelo **CountVectorizer** de extracción de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo que construiremos necesita como entrada textos, así que transformamos nuestras listas de palabras en textos, en los que las librerías estarán separados por espacios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_texts = [\" \".join(project['library']) for project in projects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tensorflow numpy hello nltk',\n",
       " 'modules argparse config requests clearbit shodan re steganography bitcoin email']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_texts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos un objeto de **CountVectorizer**, el cual convierte una colección de documentos de texto en una matriz de conteos de tokens. \n",
    "\n",
    "Useremos el parámetro **min_df**, denomina comunmente como **corte**, con el valor **2**; esto hará que al construir el vocabulario, se ignoren los términos que tienen una frecuencia de documento más baja que el umbral dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_countVectorizer = CountVectorizer(min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando el objeto creado, realizamos la transformación de nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model_countVectorizer.fit_transform(global_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación definimos una función que usando los datos anteriores, el objeto countVectorizer y una lista de librerias, calcula la distancia coseno con cada una de las listas de librerias de los proyectos objeto de estudio. La **distancia coseno**  no es propiamente una distancia sino una medida de similaridad entre dos vectores en un espacio que tiene definido un producto interior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarities_by_library(train_data, doc, model):\n",
    "    def cos_distance(v1, v2):\n",
    "        return 1 - (v1 * v2.transpose()).sum() / (sp.linalg.norm(v1.toarray()) * sp.linalg.norm(v2.toarray()))\n",
    "    \n",
    "    new_doc_vectorized = model.transform([doc])[0]\n",
    "    library_similarities = defaultdict(float)\n",
    "    for i, doc_vec in enumerate(train_data):\n",
    "        library_similarities[i] = 1 - cos_distance(doc_vec, new_doc_vectorized)\n",
    "    return library_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a realizar una prueba de concepto usando la lista de librería que ya tenemos creada, primero la convertimos en un documento tal como se ha comentado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logging time printer distutils scapy requests bs4 pysnmp paramiko nmap unittest'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_from_library = \" \".join(poc_library)\n",
    "doc_from_library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los proyectos más similares en función de las librerías que comparten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_similarities_by_library = get_similarities_by_library(data, doc_from_library, model_countVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hicimos con el análisis por descripciones, podemos mostrar los proyectos ordenados por similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: Imports-in-Python: - Similarity: nan\n",
      "Project: isf: - Similarity: 0.7071067811865475\n",
      "Project: SkySpyWatch: - Similarity: 0.4743416490252569\n",
      "Project: vulcan: - Similarity: 0.44721359549995787\n",
      "Project: rfcat-firsttry: - Similarity: 0.3651483716701107\n",
      "Project: deeppavlov: - Similarity: 0.34641016151377535\n",
      "Project: trafaret: - Similarity: 0.2981423969999719\n",
      "Project: OSINT-SPY: - Similarity: 0.2581988897471611\n",
      "Project: privacy-protocols: - Similarity: 0.2581988897471611\n",
      "Project: django_rest_example: - Similarity: 0.2581988897471611\n"
     ]
    }
   ],
   "source": [
    "show_project_similarities(projects, project_similarities_by_library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque como ya dijimos entonces, el objetivo es recomendar librerías, no proyectos. Así que obtenemos la lista a partir de nuestra lista de proyectos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_similarities_by_libraries = get_library_similarities(projects, project_similarities_by_library, poc_library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente mostramos el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library: machines: - Score: nan\n",
      "Library: re: - Score: 1.9750066011175664\n",
      "Library: datetime: - Score: 1.3770930972861652\n",
      "Library: setuptools: - Score: 1.3499650437608661\n",
      "Library: csv: - Score: 1.2679654060389902\n",
      "Library: select: - Score: 1.0722551528566582\n",
      "Library: functools: - Score: 1.0052491781865194\n",
      "Library: numbers: - Score: 1.0052491781865194\n",
      "Library: argparse: - Score: 0.9653056709337086\n",
      "Library: multiprocessing: - Score: 0.8207518105390322\n"
     ]
    }
   ],
   "source": [
    "show_library_similarities(library_similarities_by_libraries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #12297c; font-family: Arial; font-size: 3em;\">Rendimiento del modelo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divideremos el conjunto de datos en dos subconjuntos, uno de **entrenamiento** con el **70%** de los datos y otro de **test** con el **30%**. Mostramos el numero total de proyectos para verificar que se distribuyen correctamente los dos subconjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data set size: 27\n"
     ]
    }
   ],
   "source": [
    "num_projects = len(projects)\n",
    "print(\"Total data set size:\",num_projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de hacer la partición, los desordenamos, de otra forma los subconjuntos estarían sesgados dado que están ordenados por el **id** el cual depende de la fecha en la que se creó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de entrenamiento \n",
    "\n",
    "Nos quedamos con el 70% de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set size: 18\n"
     ]
    }
   ],
   "source": [
    "projects_train = projects[:int(0.7*num_projects)]\n",
    "print(\"Training data set size:\",len(projects_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de tests \n",
    "Nos quedamos con el 30% restante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data set size: 9\n"
     ]
    }
   ],
   "source": [
    "projects_test = projects[int(0.7*num_projects):]\n",
    "print(\"Test data set size:\",len(projects_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos la lista con las descripciones de los proyectos del conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cactuscon', 'http', 'static1', 'squarespac', 'com', 'static', '56b38e2aa3360ce530d55c24', '56b3fc1bf850820fe762454d', 'format', '1500w', 'http', 'cactuscon', 'com', 'us', 'major', 'updat', 'come', 'blackhat', 'arsenal', 'europ', 'stay', 'tune', 'someth', 'secret', 'osint', 'spi', 'search', 'use', 'osint', 'open', 'sourc', 'intellig', 'perform', 'osint', 'scan', 'email', 'domain', 'ip_address', 'organ', 'use', 'osint', 'spi', 'use', 'cyber', 'crime', 'investig', 'order', 'find', 'deep', 'inform', 'target', 'osint', 'spi', 'document', 'beta', 'readm', 'author', 'sk_secur', 'version', 'websit', 'osint', 'spi', 'com', 'overview', 'tool', 'scan', 'ip', 'address', 'domain', 'email', 'address', 'btc', 'bitcoin', 'address', 'devic', 'find', 'latest', 'bitcoin', 'block', 'inform', 'list', 'cipher', 'support', 'particular', 'websit', 'server', 'whether', 'particular', 'websit', 'vulner', 'heartble', 'dump', 'contact', 'messag', 'skype', 'databas', 'malwar', 'malic', 'file', 'remot', 'licens', 'inform', 'osint', 'spi', 'document', 'cover', 'gpl', 'general', 'v3', 'use', 'osint', 'spi', 'search', 'use', 'osint', 'websit', 'www', 'osint', 'spi', 'com', 'usag', 'osint', 'spi', 'py', 'option', 'option', 'h', 'help', 'show', 'help', 'messag', 'exit', 'btc_block', 'find', 'latest', 'blockchain', 'info', 'btc_date', 'blockchain', 'inform', 'given', 'date', 'btc_address', 'find', 'balanc', 'transact', 'inform', 'given', 'bitcoin', 'address', 'ssl_cipher', 'list', 'cipher', 'use', 'given', 'server', 'ssl_bleed', 'whether', 'server', 'vulner', 'heart', 'bleed', 'flaw', 'domain', 'bunch', 'detail', 'given', 'websit', 'organ', 'email', 'inform', 'given', 'email', 'address', 'devic', 'find', 'devic', 'connect', 'internet', 'ip', 'enumer', 'inform', 'given', 'ip', 'addresss', 'skype_db', 'give', 'locat', 'skype', 'databas', 'order', 'fetch', 'inform', 'includ', 'chat', 'contact', 'malwar', 'find', 'whether', 'given', 'file', 'infect', 'malwar', 'carrier', 'give', 'path', 'carrier', 'file', 'behind', 'want', 'add', 'text', 'setgo_text', 'enter', 'text', 'hide', 'behind', 'carrier', 'file', 'stego_find', 'give', 'stego', 'file', 'tri', 'find', 'hidden', 'text', 'requir', 'setup', 'python', 'use', 'install_linux', 'py', 'instal', 'depend', 'librari', 'linux', 'use', 'install_window', 'py', 'instal', 'depend', 'librari', 'window', 'contributor', 'sk_secur', 'twitter', 'handler', 'tool', 'document', 'https', 'osint', 'spi', 'com', 'command_lin', 'https', 'osint', 'spi', 'com', 'command_lin']\n"
     ]
    }
   ],
   "source": [
    "texts_train = get_texts_from_readme(projects_train)\n",
    "print(texts_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_train = corpora.Dictionary(texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1534 unique tokens: ['isweekend', 'protocol', 'accord', 'question', 'directori']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Por último creamos el corpus y el corpus tfidf a partir de él"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = create_corpus(dictionary_train, texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf_train = create_tfidf(corpus_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Con los elementos creados entrenamos el modelo a evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train = models.LsiModel(corpus_tfidf_train, id2word=dictionary_train, num_topics=TOTAL_LSA_TOPICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Creamos la matriz de similaritud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix_train = similarities.MatrixSimilarity(model_train[corpus_tfidf_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Definimos una funcion que calcule el acierto de una predicción, se basa en el porcentaje de librerias que se han predicho con respecto al total de librerías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(actual, predicted):\n",
    "    actual_len = len(actual)\n",
    "    predictec_success = set(actual).intersection(predicted)\n",
    "    predictec_success_len = len(predictec_success)\n",
    "    return(predictec_success_len/actual_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Por otra parte definimos otra función que obtiene la predicción para un proyecto y, usando la función anterior, calcula el score de dicha predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_libraries_from_description(model, dictionary, similarity_matrix, project):  \n",
    "    get_similarities_by_library(train_data, doc, model)\n",
    "    library_similarities = get_library_similarities(projects, project_similarities, [])\n",
    "    # Ordernar y pasar a lista, tomamos el mismo número de librerías que las que tiene el proyecto real\n",
    "    num_lib = len(project['library'])\n",
    "    project['library_prediction'] = sorted(library_similarities, key=library_similarities.get, reverse=True)[:num_lib]\n",
    "    project['score'] = score(project['library'], project['library_prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finalmente obtenemos la predicción para todos los proyectos del conjunto de entrenamiento, adicionalmente calculamos el score para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score = 0\n",
    "for project in projects_test:\n",
    "    predict_libraries_from_description(model_train, dictionary_train, similarity_matrix_train, project)\n",
    "    model_score += project['score']\n",
    "\n",
    "model_score = model_score/len(projects_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "El score obtenido es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26552151552151554"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cada proyecto nos quedaremos con un "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['csv', 'ortools']\n",
      "['bitcoin', 'trafaret']\n",
      "\n",
      "['argparse', 'pyqrcode', 'logging', 'zbarlight', 'hashlib', 'magic', 'fpdf']\n",
      "['machines', 'argparse', 're', 'logging', 'time', 'requests', 'hashlib']\n",
      "\n",
      "['distutils', 'serial', 'time', 'numpy', 'vstruct', 're', 'code', 'select', 'bits', 'elf', 'ctypes']\n",
      "['machines', 'time', 'tensorflow', 'numpy', 're', 'sklearn', 'argparse', 'unittest', 'keras', 'logging', 'datetime']\n",
      "\n",
      "['csv', 'redis', 'datetime', 'pymongo', 'time', 'geopy', 'pika', 'requests', 'logging', 'bson', 'geographiclib', 'multiprocessing', 'boto3']\n",
      "['time', 're', 'datetime', 'logging', 'machines', 'setuptools', 'unittest', 'requests', 'argparse', 'functools', 'numbers', 'hashlib', 'tensorflow']\n",
      "\n",
      "['numpy', 'scipy', 'lmdb', 'matplotlib']\n",
      "['machines', 'numpy', 'tensorflow', 'time']\n",
      "\n",
      "['time', 'arrow', 'requests', 'csv', 'setuptools', 'boto']\n",
      "['machines', 'time', 're', 'setuptools', 'datetime', 'argparse']\n",
      "\n",
      "['numpy', 'tensorflow', 'sonnet']\n",
      "['numpy', 'tensorflow', 'time']\n",
      "\n",
      "['numpy', 'torch', 'sklearn', 'functools']\n",
      "['machines', 'numpy', 'tensorflow', 'time']\n",
      "\n",
      "['tensorflow', 'time', 'numpy', 'helper', 're', 'scipy', 'tqdm']\n",
      "['machines', 'tensorflow', 'numpy', 'time', 'argparse', 'sklearn', 're']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "global_texts_train = [\" \".join(project['library']) for project in projects_train]\n",
    "global_texts_train[:2]\n",
    "\n",
    "model_countVectorizer = CountVectorizer(min_df=2)\n",
    "\n",
    "train_data = model_countVectorizer.fit_transform(global_texts_train)\n",
    "\n",
    "def show_library_similarities(library_similarities):\n",
    "    for library in sorted(library_similarities, key=library_similarities.get, reverse=True)[:10]:\n",
    "        print(\"Library: {0}: - Score: {1}\".format(library, library_similarities[library]))\n",
    "\n",
    "def get_similarities_by_library(train_data, doc, model):\n",
    "    def cos_distance(v1, v2):\n",
    "        return 1 - (v1 * v2.transpose()).sum() / (sp.linalg.norm(v1.toarray()) * sp.linalg.norm(v2.toarray()))\n",
    "    \n",
    "    new_doc_vectorized = model.transform([doc])[0]\n",
    "    library_similarities = defaultdict(float)\n",
    "    for i, doc_vec in enumerate(train_data):\n",
    "        library_similarities[i] = 1 - cos_distance(doc_vec, new_doc_vectorized)\n",
    "    return library_similarities\n",
    "\n",
    "\n",
    "def predict_libraries_from_library(project):\n",
    "    doc_from_library = \" \".join(project['library'])\n",
    "    project_similarities = get_similarities_by_library(train_data, doc_from_library, model_countVectorizer)\n",
    "    library_similarities = get_library_similarities(projects, project_similarities, [])\n",
    "    # Ordernar y pasar a lista, tomamos el mismo número de librerías que las que tiene el proyecto real\n",
    "    num_lib = len(project['library'])\n",
    "    return([library for library in sorted(library_similarities, key=library_similarities.get, reverse=True)[:num_lib]])\n",
    "\n",
    "\n",
    "for project in projects_test: \n",
    "    print(project['library'])\n",
    "    print(predict_libraries_from_library(project))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_libraries_from_description(model, dictionary, similarity_matrix, project):  \n",
    "    get_similarities_by_library(train_data, doc, model)\n",
    "    library_similarities = get_library_similarities(projects, project_similarities, [])\n",
    "    # Ordernar y pasar a lista, tomamos el mismo número de librerías que las que tiene el proyecto real\n",
    "    num_lib = len(project['library'])\n",
    "    project['library_prediction'] = sorted(library_similarities, key=library_similarities.get, reverse=True)[:num_lib]\n",
    "    project['score'] = score(project['library'], project['library_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<span style=\"color: #12297c; font-family: Arial; font-size: 3em;\">Aplicación a un proyecto de ejemplo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poner en práctica el sistema de recomendación vamos a usar un proyecto real almacenado en Github. En primer lugar definimos una función que dado un proyecto, lo descargue en local y aplique el mismo procesamiento que hicimos con los proyectos que componen nuestro corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proccess_url(git_url):\n",
    "    def process_python_file(project, file_path):\n",
    "        def add_to_list(item):\n",
    "            if not item in library:\n",
    "                library.append(item)\n",
    "\n",
    "        library = project['library']\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                match = re.search(LIBRARY_PATTERN, line)\n",
    "                if match:\n",
    "                    if match.group(1) is not None:\n",
    "                        add_to_list(match.group(1))\n",
    "                    else:\n",
    "                        add_to_list(match.group(2))\n",
    "        project['library'] = library\n",
    "\n",
    "    def process_readme_file(project, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            project['readme_txt'] = f.read()\n",
    "        project['readme_words'] = get_words(project['readme_txt'])\n",
    "\n",
    "    project = dict()\n",
    "    os.chdir(ROOT_PATH)\n",
    "    dir_name = uuid.uuid4().hex\n",
    "    path = ROOT_PATH + \"/\" + dir_name\n",
    "\n",
    "    if not os.path.isdir(path):\n",
    "        os.system(CLONE_COMMAND.format(git_url, dir_name))\n",
    "        project['git_url'] = git_url\n",
    "        project['library'] = []\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                try:\n",
    "                    if file.endswith(\".py\"):\n",
    "                        process_python_file(project, os.path.join(root, file))\n",
    "                    else:\n",
    "                        if file.lower().startswith(\"readme.\"):\n",
    "                            process_readme_file(project, os.path.join(root, file))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    return project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la invocamos con nuestro projecto de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_project = proccess_url(\"https://github.com/yazquez/programmable-agents_tensorflow.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las librerías que está usando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_project['library'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así como su descripción (fichero **readme**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_project['readme_txt'][:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la descomposición de dicho fichero en sus palabras significativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_project['readme_words'][:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si recapitulamos, tenemos las siguientes funciones definidas:\n",
    "\n",
    "- **get_similarities_by_description**: Devuelve los proyectos similares en funcíon de sus descripciones\n",
    "- **get_similarities_by_library**: Devuelve los proyectos similares en funcíon de las librerías que se usa\n",
    "- **get_library_similarities(project_similarities)**: Devuelve las librerías comunes usando una lista de proyectos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las aplicamos a nuestro proyecto de ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos obteniendo los proyectos más similares al del ejemplo usando las descripciones y a partir de estos proyectos obtenemos las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_similarities_by_description = get_similarities_by_description(model, dictionary, similarity_matrix, sample_project['readme_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_similarities_by_description = get_library_similarities(projects, project_similarities_by_description, sample_project['library'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos los 10 proyectos más similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_project_similarities(projects, project_similarities_by_library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos lo mismo usando las librerías del proyecto como medida de similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_similarities_by_library = get_similarities_by_library(X_train_data, \" \".join(sample_project['library']), countVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_similarities_by_libraries = get_library_similarities(projects, project_similarities_by_library, sample_project['library'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos los 10 proyectos más similares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_project_similarities(projects, project_similarities_by_library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente mostramos el resultado, o sea las librerías recomendadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librerías recomendadas en función de las descripciones**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_library_similarities(library_similarities_by_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Librerías recomendadas en función de las librerías comunes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_library_similarities(library_similarities_by_libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
